记录实验设置：
    sample数据集39号场景：
        2023-02-07_16-05：纯PPO，loss = policy_loss + 0.0 * entropy_loss + 0.5 * value_loss；不能处理路口转弯。
        2023-02-13_10-09：PPO但读取了IL数据，loss = 0.0 + policy_loss + 0.0 * entropy_loss + 0.005 * value_loss；停在300k，路口转弯效果不好。
        2023-02-13_10-46：一步IL(MSE)+PPO，loss = 1.0 * il_loss + policy_loss + 0.0 * entropy_loss + 0.005 * value_loss；有一定效果，不过应该不如单场景纯IL，依旧不能很好地在路口转弯行驶。
        2023-02-13_15-59：一步IL(MSE)，loss = 1.0 * il_loss + 0.0*(policy_loss + 0.0 * entropy_loss + 0.005 * value_loss)；为了验证是不是因为IL是对actions_mean训练而不是采样后的actions而导致效果不好；或者ILloss本身存在问题？
        2023-02-13_16-51:一步IL(MSE)，loss相同，但是是对采样后的最终actions计算il_loss，还是不行，loss下降但是reward和闭环测试表现差，会不会是action不一样格式的问题？经验证，直接用target actions控制也不能跑好，应该就是用于il训练的target不对。
        
        后再经调试证实，是由于l5_env.py line:291 action = self._rescale_action(action)考虑到PPO[-1,1]的动作空间限制，在环境内仿真时擅自进行了额外的rescale，导致直接从数据集读取的target不再正确。由于不方便从SubprocVecEnv环境读取rescale参数，因此只能考虑放弃rescale并适当放宽PPO action_space。※实际上，[-1,1]的完全够用，不会clip掉取值。以下是非rescale的：

        2023-02-14_15-50：以action_mean模仿，std下降，一步IL(MSE)+PPO，loss = 1.0 * il_loss + policy_loss + 0.0 * entropy_loss + 0.005 * value_loss；效果还是不太好，直接取消rescale会对PPO造成较大影响，不好训练，必须想办法得到rescale参数从而对il的target进行修改！
        2023-02-14_16-10：以action模仿，std下降，一步IL(MSE)，loss = 1.0 * il_loss + 0.0*(policy_loss + 0.0 * entropy_loss + 0.005 * value_loss)；同上。

        2023-02-15_09-28：rescale版本，以action_mean模仿，一步IL(MSE)+PPO，loss = 1.0 * il_loss + policy_loss + 0.0 * entropy_loss + 0.005 * value_loss；由于下一条所述的纯模仿都没能实现，混合效果自然无法提升。
        2023-02-15_09-30：rescale版本，以action模仿，一步IL(MSE)，loss = 1.0 * il_loss + 0.0*(policy_loss + 0.0 * entropy_loss + 0.005 * value_loss)；还是不能实现路口转弯，可能是由于一步模仿不容易学习？
        2023-02-15_16-01：rescale版本，以action模仿，deterministic=True，一步IL(MSE)，loss = 10.0 * il_loss + 0.0*(policy_loss + 0.0 * entropy_loss + 0.005 * value_loss)；
        2023-02-15_18-35：rescale版本，以action模仿，加扰动，一步IL(MSE)，loss = 10.0 * il_loss + 0.0*(policy_loss + 0.0 * entropy_loss + 0.005 * value_loss)；

        2023-02-15_19-50：rescale版本，以action模仿，不加扰动，一步IL(MSE)，50 纯IL_n_epochs + 50 混合RL_n_epochs，相当于增加了IL的比重，混合loss = 10.0 * il_loss + 0.0*(policy_loss + 0.0 * entropy_loss + 0.005 * value_loss)；有没有一种可能，是单独39号场景不太好训练？从闭环测试中可以看出，智能车在转弯的那几帧action loss较大，但是在前后由于都是跑直线action loss其实不大，因此或许是在这样的图像观测和单场景训练集下，学会在那里转弯真的太难了？对于这个39号场景的246帧构成的数据集只有30帧是路口转弯...解决办法：1.换一个稍微简单的弯道场景再验证一下！ 2.直接用原来的轨迹规划网络试一次39号场景。
        
        经过验证，不是网络模型的问题，是规划步数的问题！！！12步能够学到预测未来的轨迹，因此比1步学习的更善于转弯。

        2023-02-20_16-59：12步轨迹预测模仿训练+模仿奖励强化学习训练，256*4步collects+128步eps_length；loss波动大需要调整分开loss记录，action net, pred net, 其他RL loss各自记录；此外训练结构也不够好，40次RL+60次Pred训练突变太大。

        更改了循环训练结构为10epochs每个epoch一次RL一次IL，并换用预训练的resnet50作为Feature Extractor。此前il loss波动不下降其实是因为pred_net添加的参数没有被optimizer纳入考虑！！！2023-02-21_14-03

        2023-02-22_12-19，2023-02-22_13-10，训练时用[batch_size, future_num_frames*3]的方式乘上availabilities求loss，并且rescale相当于只对第1步的进行，后面11步轨迹训练时不rescale target。
        2023-02-22_15-17，直接只用resnet50网络的效果可以，原本的action net涉及的概率采样可能有点问题。
        2023-02-22_17-53，为了验证上一条，在resnet50网络的基础上把其他几层网络加上但是不用action dist采样，并且降低了学习率为3e-4，看看效果。效果很好，证明多加几层网络不影响轨迹学习，而且由于调低了学习率还使学习变快，750个updates就有沿着车道开的相当不错的效果！！！下面只需要验证action dist的部分是直接去掉还是添加logit的loss。

        2023-02-23_16-44，12步pred+PPO，deterministic=True，可能是由于Value Net 和 Pred共用了特征提取网络？还是由于Value Net训练频率太低没收敛，总之结果是先经历了一个变差的过程。2023-02-23_18-35，Value Net同步和Pred Net一直训练，有一定效果，但有限。2023-02-23_19-02，在上基础上分离了Value和Policy共用的特征提取网络，各自用一个Resnet 50。2023-02-23_19-35，在上基础上缩短episode长度由128至32（默认）。都不行，是因为不好的value估计对action net以及前面一系列特征提取网络的更新影响太大了吗？？
        如果不共用特征提取网络且不更新action网络：2023-02-24_10-04，效果和纯预测一致，是时候改奖励了？

        ！！！！！大无语事件，原来我的仿真环境不是固定在39号而是从39号开始循环啊！怪不得IL总是出问题......我早该注意到即使是闭环测试效果很好的模型在训练的仿真中也不能够有相应的好的评价指标......但是这个影响或许没有那么大，因为IL取39号场景的数据进行训练是独立进行的。充其量只是说IL只在这里发挥作用，而对于纯预测的算法由于环境本身对于它没什么实质性的作用，则更是不受影响。因此推断如下结论都是依旧正确的：1、单场景12步预测比1步要好（在原本默认的网络中验证的）；2、对于纯预测算法，相当于在39训练、在39测试，其前期（750-1000迭代）仍旧存在一定的问题是肯定的。而受到影响的主要是：1、之前PPO算法一直性能有限，可能是受到环境变动之后不好训练的影响，也就是说，在39号外的场景上没有预测的辅助而只依靠它自己进行训练，导致它在39号性能较差，即使此时在39号场景中有预测算法的辅助；2、可视化训练过程中的平均奖励、ade、fde等指标错误，说明不了任何和训练有关的问题；3、此外还有就是会影响来自env的rescale放缩！！
        tb_log_2023-02-24_15-49：修正后第一次验证，效果直接得到提升，不过有个问题是随着训练效果变差了。
        一个疑点：为什么ep的长度（32或128）会这么严重地影响训练效果？

        (==== STAR ====)
        加扰动单场景纯12步预测：2023-02-27_10-15，模型在models_net3，画图文件是il3系列，加了扰动之后效果特别好，单场景1000步就可以做到3的ade fde误差（但是1250的误差又有所增长到12，而且发生了严重的碰撞，1500为5，说明波动还是存在的，只是Pred是不够的）！！！基本驾驶非常顺利，就是有一个特点：有时候还是离其他车辆比较近，没有很好的躲避障碍的意识。那么加扰动的1步预测呢？
        2023-02-27_11-15，加扰动单场景纯1步预测，模型在models_net4，画图文件是il4系列，效果远不如12步预测，不善学习转弯。
        阶段结论：加扰动能够有效地使自车学会在道路边缘向中心靠近从而避免外延误差；多步轨迹预测相比一步模仿能帮助自车更有效地学会转弯等动作，因为其考虑了未来的轨迹。
        2023-02-27_14-47：使用上action net（并改良了loss mean但是这一版的不能看），由于网络变复杂学习的速率稍慢了些，但事实上单场景纯预测只要一直训练是可以最终达到一个较好的模仿效果的。
        2023-02-27_16-06：加上PPO部分，目前从效果来看还是需要改训练方式，最好还是像参考论文中一样混合成一个Loss训练，并且先换上简单的奖励函数（比如碰撞距离奖励）。

        2023-03-01_16-00：在好的纯预测基础上，按参考论文方式回归了原本的训练方式，并且换了碰撞车距奖励函数。效果非常好！后期可以加（1）off-road奖励再和只有避碰的对比，（2）多步避碰奖励和只有一步避碰奖励在预判轨迹和其他车辆进行避碰的情况下对比，比如提前规划远离某车。其中，基于预测自车12步轨迹进行预测避碰时，他车可以先直接考虑将按当前速度运行得到轨迹点求最小距离。
        至此，单场景基础实验基本实现。
        2023-03-02_17-30：添加了和道路中线距离的off-road奖励项，进行训练。1250 iters效果还可以，一定程度解决了off-road问题（最明显的在路口转弯那里），但是相应的对其他车的规避动作不再明显；或许后面的实验中需要把off-road的比例调小些，或者对远离中线的惩罚阈值增大些，以抑制误判其他路中线导致抖动或干扰比避碰。

    validate数据集前1000场景训练，sample数据集测试：
        2023-03-04_09-57，39号场景效果不好，pred12部分的loss下降的很慢。或许应该调高学习率(6e-5)，或者用退火学习率？亦或是奖励函数不好，需要先换成简单避碰奖励试试？还是训练方式有问题，在扩大至1000场景训练的同时应该使pred12部分更多地训练？用上更好的网络结构？还是单纯学习变难，12小时才跑5000步并且并不足以学到东西？那又怎么解释RL部分的loss的大幅波动？
        2023-03-05_10-17，先试试Pred12的算法，3e-4学习率，预测的loss下降更慢了；39号场景测试效果也不好，但是是会沿道路走直线的，所以需要输入转向信息才好？其他场景，虽然沿车道运行，但是速度特别快是为何？还是因为24k步0.1的loss也还没学完吗？学习率调大有用吗？  -有办法用预训练模型吗？-  更复杂的网络、向量化输入会有效吗？
        会不会是rescale本身有问题了？训练集和测试集不一样，导致rescale参数不一样，跑的更快？优先验证。
        经过验证，eval文件所读取的get_il_data读成了训练集，修正后直接从验证集相应场景读动作控制是可以的。但是有趣的是，在训练集测试的效果非常好，那么是哪里出的问题？
        是rescale不一致的问题！！！需要把训练集(validate.zarr)的补充上，把测试集(sample.zarr)的抵消掉。在这样操作后，效果非常好。能够在测试集12号、25号场景等红灯（12号出现微漂移），在13、15号场景绿灯启动（13号碰撞停车）；能在39号场景学到转弯（但是仍然存在碰撞和off road），

        下一步实验方向：加上强化学习奖励，重点看类似39号场景的提升效果。如果觉得从头训练加上强化比较慢，可以考虑基于预训练从某保存点开始加强化训练。注意：后续转向信号估计还是要加的。

        2023-03-07_16-52：加上了强化学习奖励，利用50000迭代的12步预测模型作预训练；一开始il loss波动大；optimizer没用预训练的会有影响吗？不行再验证简化奖励函数。训练效率太低，il loss波动大，9000迭代后学习效果有限。没有在加载预训练模型参数后加载optimizer，其实相当于没有更新optimizer的参数
        注：PPO等一众on policy算法更适合做大规模预训练后的在线微调，直接同步训练则效率低下；而SAC等off policy算法乃至一众off line算法，更加适合联合监督学习从头训练。有机会的话，一定要在后面的工作中试试off line算法。
        2023-03-08_11-32，用重新build optimizer加载新参数的方式继续训练，无效。解决办法有3: (0) 重新按action_net拼接pred_net的方式训练一次纯12步预测； (1) 12维pred_net的第一维后接action_net，这样改变网络结构；(2) 直接从零训练，每250个il 迭代作50次 rl训练，并进行一次在线采样。
        2023-03-08_14-57，为方便复用模型和优化器参数而重新训练的il3_1000的action+pred网络拼接版本，起名il4_1000，模型储存在models_net4_1000，然而还是不能直接加载优化器的params参数。先暂停了训练。
        (0)(1)不行，调小学习率再接入预训练？或者降低RL loss初始比例？究其原因应该还是未加RL loss和加了之后的数量级上的loss差异带来的扰动和破坏。
        2023-03-08_17-40：(2) 直接从零训练，每250个il 迭代作50次 rl训练，并进行一次在线采样。太慢了！
        找到慢的重大原因了！是在for循环时若是每次从rollout_buffer里面取数据则会非常慢，这一点需要优化！还有，在此基础上需要试一下加载模型的时候选择share features extractor 网络是否就可以加载预训练模型和优化器了呢？是的！

        2023-03-09_15-35：直接从零训练，每epoch先把rollout_buffer数据拿出来再依次读取，而不是每个循环都调用取数据函数。训练速度得到提升（3 fps），相比原本纯预测的（5 fps）比较接近了。20k步后总训练变慢，value loss不降反增，可能是二者不能更进一步地兼容了？
        此外，如有需要，也可以通过共用特征提取层的方式再预训练模型的基础上加强化学习模块。
        2023-03-10_11-03：直接从零训练，每epoch先把rollout_buffer数据拿出来再依次读取，调整参数ent_coef = 0.01, vf_coef = 0.1,收敛的还是效果不好，到0.1左右il loss基本就达到训练瓶颈，很难再继续优化。方案：（1）保守：换成原本的非共享特征提取网络，但是比较慢，先保证有一个最低效果: 2023-03-10_15-09；（2）创新：仿照之前的工作加一个Transformer编解码层作特征提取用（可以作改进点）
        2023-03-12_10-23：最低效果也不能保证，可能还是因为RL的reward训练方式在大场景中比较难优化，为调试，将道路中线奖励去掉而仅保留避碰距离奖励，并且调小每次训练epochs数目间隔到50。

        根据现有的实验现象，在复杂大场景训练时，强化学习和模仿学习可能发生冲突，解决方案：（1）不从scrach开始学，而是利用预训练简化一些训练难度（2）更完善的奖励函数，比如更好的车道中心保持奖励或者加入一定的模仿学习奖励（3）升级网络结构

        2023-03-13_11-26:（1）不从scrach开始学，而是利用预训练简化一些训练难度。这是预训练部分，保存在models_pretrain_1000
        2023-03-14_18-11: 基于100000纯预测预训练模型的预测+强化。2023-03-14_21-30变为一次25 epochs训练并简化奖励。