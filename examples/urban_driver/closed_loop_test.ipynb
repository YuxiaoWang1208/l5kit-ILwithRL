{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed-Loop Evaluation\n",
    "In this notebook you are going to evaluate Urban Driver to control the SDV with a protocol named *closed-loop* evaluation.\n",
    "\n",
    "**Note: this notebook assumes you've already run the [training notebook](./train.ipynb) and stored your model successfully (or that you have stored a pre-trained one).**\n",
    "\n",
    "**Note: for a detailed explanation of what closed-loop evaluation (CLE) is, please refer to our [planning notebook](../planning/closed_loop_test.ipynb)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project path:  /mnt/share_disk/user/xijinhao/l5kit-model-based-offline-rl\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "\n",
    "from l5kit.dataset import EgoDatasetVectorized\n",
    "from l5kit.vectorization.vectorizer_builder import build_vectorizer\n",
    "\n",
    "from l5kit.rasterization.rasterizer_builder import build_rasterizer\n",
    "from l5kit.simulation.dataset import SimulationConfig\n",
    "from l5kit.simulation.unroll import ClosedLoopSimulator\n",
    "from l5kit.cle.closed_loop_evaluator import ClosedLoopEvaluator, EvaluationPlan\n",
    "from l5kit.cle.metrics import (CollisionFrontMetric, CollisionRearMetric, CollisionSideMetric,\n",
    "                               DisplacementErrorL2Metric, DistanceToRefTrajectoryMetric)\n",
    "from l5kit.cle.validators import RangeValidator, ValidationCountingAggregator\n",
    "\n",
    "from l5kit.visualization.visualizer.zarr_utils import simulation_out_to_visualizer_scene\n",
    "from l5kit.visualization.visualizer.visualizer import visualize\n",
    "from bokeh.io import output_notebook, show\n",
    "from l5kit.data import MapAPI\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append('/mnt/share_disk/user/xijinhao/l5kit-model-based-offline-rl/scripts')\n",
    "sys.path.append('/mnt/share_disk/user/xijinhao/l5kit-model-based-offline-rl/scripts/vectorized_offline_rl_model')\n",
    "sys.path.append('/mnt/share_disk/user/xijinhao/l5kit-model-based-offline-rl/scripts/reward')\n",
    "import vectorized_offline_rl_model\n",
    "from vectorized_offline_rl_model import VectorOfflineRLModel, EnsembleOfflineRLModel\n",
    "\n",
    "from l5kit.planning.vectorized.closed_loop_model import VectorizedUnrollModel  \n",
    "from l5kit.planning.vectorized.open_loop_model import VectorizedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data path and load cfg\n",
    "\n",
    "By setting the `L5KIT_DATA_FOLDER` variable, we can point the script to the folder where the data lies.\n",
    "\n",
    "Then, we load our config file with relative paths and other configurations (rasteriser, training params ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project path:  /mnt/share_disk/user/xijinhao/l5kit-model-based-offline-rl\n",
      "{'format_version': 7, 'model_params': {'history_num_frames_ego': 1, 'history_num_frames': 0, 'history_num_frames_agents': 3, 'step_time': 0.1, 'disable_other_agents': False, 'disable_map': False, 'disable_lane_boundaries': True, 'global_head_dropout': 0.0, 'future_num_frames': 12, 'detach_unroll': True, 'warmup_num_frames': 0, 'discount_factor': 0.8, 'render_ego_history': True}, 'train_data_loader': {'key': 'scenes/train.zarr', 'batch_size': 16, 'pred_len': 10, 'shuffle': True, 'num_workers': 2, 'perturb_probability': 0.5, 'yaml': None}, 'val_data_loader': {'key': 'scenes/train.zarr', 'batch_size': 16, 'shuffle': False, 'num_workers': 2}, 'raster_params': {'raster_size': [224, 224], 'pixel_size': [0.5, 0.5], 'ego_center': [0.5, 0.5], 'map_type': 'py_satellite', 'satellite_map_key': 'aerial_map/aerial_map.png', 'semantic_map_key': 'semantic_map/semantic_map.pb', 'dataset_meta_key': 'meta.json', 'filter_agents_threshold': 0.8, 'disable_traffic_light_faces': False, 'set_origin_to_bottom': True}, 'data_generation_params': {'other_agents_num': 30, 'max_agents_distance': 35, 'lane_params': {'max_num_lanes': 30, 'max_points_per_lane': 20, 'max_points_per_crosswalk': 20, 'max_retrieval_distance_m': 35, 'max_num_crosswalks': 20}}, 'train_params': {'checkpoint_every_n_steps': 500, 'max_num_steps': 100000000.0, 'eval_every_n_steps': 500}}\n"
     ]
    }
   ],
   "source": [
    "project_path = \"/mnt/share_disk/user/xijinhao/l5kit-model-based-offline-rl\"\n",
    "print(\"project path: \", project_path)\n",
    "sys.path.append(project_path)\n",
    "\n",
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"/mnt/share_disk/user/public/l5kit/prediction\"\n",
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "cfg = load_config_data(str(Path(project_path, \"scripts/offline_rl_config.yaml\")))\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f461e0322e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Offline RL Planner\"\n",
    "weights_scaling = [1.0, 1.0, 1.0]\n",
    "_num_predicted_frames = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "_num_predicted_params = len(weights_scaling)\n",
    "kwargs=dict(\n",
    "            history_num_frames_ego=cfg[\"model_params\"][\"history_num_frames_ego\"],\n",
    "            history_num_frames_agents=cfg[\"model_params\"][\"history_num_frames_agents\"],\n",
    "            num_targets=_num_predicted_params * _num_predicted_frames,\n",
    "            weights_scaling=weights_scaling,\n",
    "            criterion=torch.nn.L1Loss(reduction=\"none\"),\n",
    "            global_head_dropout=cfg[\"model_params\"][\"global_head_dropout\"],\n",
    "            disable_other_agents=cfg[\"model_params\"][\"disable_other_agents\"],\n",
    "            disable_map=cfg[\"model_params\"][\"disable_map\"],\n",
    "            disable_lane_boundaries=cfg[\"model_params\"][\"disable_lane_boundaries\"],\n",
    "            cfg=cfg\n",
    ")\n",
    "\n",
    "\n",
    "num_ensemble = 2\n",
    "model_list = [VectorOfflineRLModel(**kwargs) for _ in range(num_ensemble)]\n",
    "\n",
    "model_path0=\"/mnt/share_disk/user/xijinhao/l5kit-model-based-offline-rl/tmpperturb_2_13_0.3/Offline RL Planner-train_flag_0signal_scene_13-il_weight_1.0-pred_weight_1.0-pretrained_True-1/iter_0040000.pt\"\n",
    "model_path1=\"/mnt/share_disk/user/xijinhao/l5kit-model-based-offline-rl/tmpperturb_2_13_0.3/Offline RL Planner-train_flag_0signal_scene_13-il_weight_1.0-pred_weight_1.0-pretrained_True-1/iter_0040000.pt\"\n",
    "model_path2=\"/mnt/share_disk/user/xijinhao/l5kit-model-based-offline-rl/tmphistory_10_perturb_2_1000_0.3/Offline RL Planner-train_flag_0signal_scene_0-il_weight_1.0-pred_weight_1.0-pretrained_True-1/iter_0020000.pt\"\n",
    "model_path3=\"/mnt/share_disk/user/xijinhao/l5kit-model-based-offline-rl/tmphistory_10_perturb_2_1000_0.3/Offline RL Planner-train_flag_0signal_scene_0-il_weight_1.0-pred_weight_1.0-pretrained_True-1/iter_0020000.pt\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_list[0].load_state_dict(torch.load(model_path0))\n",
    "model_list[1].load_state_dict(torch.load(model_path1))\n",
    "# model_list[2].load_state_dict(torch.load(model_path2))\n",
    "# model_list[3].load_state_dict(torch.load(model_path3))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EnsembleOfflineRLModel(model_list)\n",
    "model=model.to(device)\n",
    "model = model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Offline RL Planner\"\n",
    "num_ensemble = 4\n",
    "model_list = [model for _ in range(num_ensemble)]\n",
    "model = EnsembleOfflineRLModel(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mbop\n",
    "model_path=\"/mnt/share_disk/user/daixingyuan/l5kit/tmp/Offline RL Planner-signal_scene_13-il_weight_1.0-pred_weight_1.0-1/iter_0025000.pt\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model=model.to(device)\n",
    "model = model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline urban_driver  toch_static\n",
    "weights_scaling = [1.0, 1.0, 1.0]\n",
    "_num_predicted_frames = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "_num_predicted_params = len(weights_scaling)\n",
    "import torch.nn as nn\n",
    "\n",
    "model = VectorizedUnrollModel(\n",
    "    history_num_frames_ego=cfg[\"model_params\"][\"history_num_frames_ego\"],\n",
    "    history_num_frames_agents=cfg[\"model_params\"][\"history_num_frames_agents\"],\n",
    "    num_targets=_num_predicted_params * _num_predicted_frames,\n",
    "    weights_scaling=weights_scaling,\n",
    "    criterion=nn.L1Loss(reduction=\"none\"),\n",
    "    global_head_dropout=cfg[\"model_params\"][\"global_head_dropout\"],\n",
    "    disable_other_agents=cfg[\"model_params\"][\"disable_other_agents\"],\n",
    "    disable_map=cfg[\"model_params\"][\"disable_map\"],\n",
    "    disable_lane_boundaries=cfg[\"model_params\"][\"disable_lane_boundaries\"],\n",
    "    detach_unroll=cfg[\"model_params\"][\"detach_unroll\"],\n",
    "    warmup_num_frames=cfg[\"model_params\"][\"warmup_num_frames\"],\n",
    "    discount_factor=cfg[\"model_params\"][\"discount_factor\"],\n",
    ")\n",
    "\n",
    "model_path = \"/mnt/share_disk/user/xijinhao/l5kit-model-based-offline-rl/tmpurban_driver/Urban Driver-train_flag_0signal_scene_13-il_weight_1.0-pred_weight_1.0-1/iter_0773000.pt\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=model.to(device)\n",
    "model = model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline urban_driver\n",
    "# model_path = \"/mnt/share_disk/user/xijinhao/l5kit-model-based-offline-rl/examples/urban_driver/MS.pt\"\n",
    "model_path = \"/mnt/share_disk/user/xijinhao/l5kit-model-based-offline-rl/tmpurban_driver/Urban Driver-train_flag_0signal_scene_13-il_weight_1.0-pred_weight_1.0-1/iter_0332000.pt\"\n",
    "model=torch.load(model_path)\n",
    "model = model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline urban_driver_without_BPTT\n",
    "model_path = \"/mnt/share_disk/user/xijinhao/l5kit-model-based-offline-rl/examples/urban_driver/BPTT.pt\"\n",
    "model=torch.load(model_path)\n",
    "model = model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline open loop torch_static\n",
    "weights_scaling = [1.0, 1.0, 1.0]\n",
    "_num_predicted_frames = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "_num_predicted_params = len(weights_scaling)\n",
    "import torch.nn as nn\n",
    "\n",
    "model = VectorizedModel(\n",
    "            history_num_frames_ego=cfg[\"model_params\"][\"history_num_frames_ego\"],\n",
    "            history_num_frames_agents=cfg[\"model_params\"][\"history_num_frames_agents\"],\n",
    "            num_targets=_num_predicted_params * _num_predicted_frames,\n",
    "            weights_scaling=weights_scaling,\n",
    "            criterion=nn.L1Loss(reduction=\"none\"),\n",
    "            global_head_dropout=cfg[\"model_params\"][\"global_head_dropout\"],\n",
    "            disable_other_agents=cfg[\"model_params\"][\"disable_other_agents\"],\n",
    "            disable_map=cfg[\"model_params\"][\"disable_map\"],\n",
    "            disable_lane_boundaries=cfg[\"model_params\"][\"disable_lane_boundaries\"],\n",
    "        )\n",
    "\n",
    "model_path = \"/mnt/share_disk/user/xijinhao/l5kit-model-based-offline-rl/tmpopen_loop_planner/Open Loop Planner-train_flag_0signal_scene_13-il_weight_1.0-pred_weight_1.0-1/iter_1473000.pt\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=model.to(device)\n",
    "model = model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline open loop\n",
    "model_path = \"/mnt/share_disk/user/xijinhao/l5kit-model-based-offline-rl/examples/urban_driver/OL.pt\"\n",
    "model=torch.load(model_path)\n",
    "model = model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline open loop with history\n",
    "model_path = \"/mnt/share_disk/user/xijinhao/l5kit-model-based-offline-rl/examples/urban_driver/OL_HS.pt\"\n",
    "model=torch.load(model_path)\n",
    "model = model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the evaluation data\n",
    "Differently from training and open loop evaluation, this setting is intrinsically sequential. As such, we won't be using any of PyTorch's parallelisation functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   16265    |  4039527   | 320124624  |    38735988   |      112.19     |        248.36        |        79.25         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "# ===== INIT DATASET\n",
    "eval_cfg = cfg[\"val_data_loader\"]\n",
    "eval_zarr = ChunkedDataset(dm.require(eval_cfg[\"key\"])).open()\n",
    "vectorizer = build_vectorizer(cfg, dm)\n",
    "eval_dataset = EgoDatasetVectorized(cfg, eval_zarr, vectorizer)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some simulation properties\n",
    "We define here some common simulation properties such as the length of the simulation and how many scene to simulate.\n",
    "\n",
    "**NOTE: these properties have a significant impact on the execution time. We suggest you to increase them only if your setup includes a GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_scenes_to_unroll = 10\n",
    "num_simulation_steps = 248"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed-loop simulation\n",
    "\n",
    "We define a closed-loop simulation that drives the SDV for `num_simulation_steps` steps while using the log-replayed agents.\n",
    "\n",
    "Then, we unroll the selected scenes.\n",
    "The simulation output contains all the information related to the scene, including the annotated and simulated positions, states, and trajectories of the SDV and the agents.  \n",
    "If you want to know more about what the simulation output contains, please refer to the source code of the class `SimulationOutput`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== DEFINE CLOSED-LOOP SIMULATION\n",
    "sim_cfg = SimulationConfig(use_ego_gt=False, use_agents_gt=True, disable_new_agents=True,\n",
    "                           distance_th_far=500, distance_th_close=50, num_simulation_steps=num_simulation_steps,\n",
    "                           start_frame_index=0, show_info=True)\n",
    "\n",
    "sim_loop = ClosedLoopSimulator(sim_cfg, eval_dataset, device, model_ego=model, model_agents=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/share_disk/user/xijinhao/l5kit/l5kit/l5kit/data/zarr_dataset.py:213: RuntimeWarning: zarr dataset path should end with .zarr (for now). Open will fail for this dataset!\n",
      "  dataset = ChunkedDataset(\"\")\n",
      "/mnt/share_disk/user/xijinhao/l5kit/l5kit/l5kit/simulation/utils.py:107: RuntimeWarning: zarr dataset path should end with .zarr (for now). Open will fail for this dataset!\n",
      "  new_dataset = ChunkedDataset(\"\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ecb6e0dcbe43858ca60b1a69521b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'axis' is an invalid keyword argument for sum()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ==== UNROLL\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# scenes_to_unroll = list(range(0, len(eval_zarr.scenes), len(eval_zarr.scenes)//num_scenes_to_unroll))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m scenes_to_unroll\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m sim_outs \u001b[38;5;241m=\u001b[39m \u001b[43msim_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munroll\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/xijinhao/l5kit/l5kit/l5kit/simulation/unroll.py:213\u001b[0m, in \u001b[0;36mClosedLoopSimulator.unroll\u001b[0;34m(self, scene_indices)\u001b[0m\n\u001b[1;32m    211\u001b[0m ego_input \u001b[38;5;241m=\u001b[39m sim_dataset\u001b[38;5;241m.\u001b[39mrasterise_frame_batch(frame_index)\n\u001b[1;32m    212\u001b[0m ego_input_dict \u001b[38;5;241m=\u001b[39m default_collate(ego_input)\n\u001b[0;32m--> 213\u001b[0m ego_output_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_ego\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmove_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mego_input_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m ego_input_dict \u001b[38;5;241m=\u001b[39m move_to_numpy(ego_input_dict)\n\u001b[1;32m    216\u001b[0m ego_output_dict \u001b[38;5;241m=\u001b[39m move_to_numpy(ego_output_dict)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/xijinhao/l5kit-model-based-offline-rl/scripts/vectorized_offline_rl_model.py:51\u001b[0m, in \u001b[0;36mEnsembleOfflineRLModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     49\u001b[0m first_step, trajectory, one_step_planning, one_step_other_agents_prediction, all_trajectory_and_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmpc(data)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrajectory_value_list\u001b[38;5;241m.\u001b[39mappend(all_trajectory_and_value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrajectory_value\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum_trajectory_value\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrajectory_value_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m first_step\n",
      "\u001b[0;31mTypeError\u001b[0m: 'axis' is an invalid keyword argument for sum()"
     ]
    }
   ],
   "source": [
    "# ==== UNROLL\n",
    "# scenes_to_unroll = list(range(0, len(eval_zarr.scenes), len(eval_zarr.scenes)//num_scenes_to_unroll))\n",
    "scenes_to_unroll=list(range(15))\n",
    "sim_outs = sim_loop.unroll([13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed-loop metrics\n",
    "\n",
    "**Note: for a detailed explanation of CLE metrics, please refer again to our [planning notebook](../planning/closed_loop_test.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [DisplacementErrorL2Metric(),\n",
    "           DistanceToRefTrajectoryMetric(),\n",
    "           CollisionFrontMetric(),\n",
    "           CollisionRearMetric(),\n",
    "           CollisionSideMetric()]\n",
    "\n",
    "validators = [RangeValidator(\"displacement_error_l2\", DisplacementErrorL2Metric, max_value=30),\n",
    "              RangeValidator(\"distance_ref_trajectory\", DistanceToRefTrajectoryMetric, max_value=4),\n",
    "              RangeValidator(\"collision_front\", CollisionFrontMetric, max_value=0),\n",
    "              RangeValidator(\"collision_rear\", CollisionRearMetric, max_value=0),\n",
    "              RangeValidator(\"collision_side\", CollisionSideMetric, max_value=0)]\n",
    "\n",
    "intervention_validators = [\"displacement_error_l2\",\n",
    "                           \"distance_ref_trajectory\",\n",
    "                           \"collision_front\",\n",
    "                           \"collision_rear\",\n",
    "                           \"collision_side\"]\n",
    "\n",
    "cle_evaluator = ClosedLoopEvaluator(EvaluationPlan(metrics=metrics,\n",
    "                                                   validators=validators,\n",
    "                                                   composite_metrics=[],\n",
    "                                                   intervention_validators=intervention_validators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative evaluation\n",
    "\n",
    "We can now compute the metric evaluation, collect the results and aggregate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cle_evaluator.evaluate(sim_outs)\n",
    "validation_results = cle_evaluator.validation_results()\n",
    "agg = ValidationCountingAggregator().aggregate(validation_results)\n",
    "cle_evaluator.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting errors from the closed-loop\n",
    "\n",
    "We can now report the metrics and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\"metric\", \"value\"]\n",
    "table = PrettyTable(field_names=fields)\n",
    "\n",
    "values = []\n",
    "names = []\n",
    "\n",
    "for metric_name in agg:\n",
    "    table.add_row([metric_name, agg[metric_name].item()])\n",
    "    values.append(agg[metric_name].item())\n",
    "    names.append(metric_name)\n",
    "\n",
    "print(table)\n",
    "\n",
    "plt.bar(np.arange(len(names)), values)\n",
    "plt.xticks(np.arange(len(names)), names, rotation=60, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the closed-loop\n",
    "\n",
    "We can visualise the scenes we have obtained previously. \n",
    "\n",
    "**The policy is now in full control of the SDV as this moves through the annotated scene.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "mapAPI = MapAPI.from_cfg(dm, cfg)\n",
    "for sim_out in sim_outs: # for each scene\n",
    "    vis_in = simulation_out_to_visualizer_scene(sim_out, mapAPI)\n",
    "    show(visualize(sim_out.scene_id, vis_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ScePT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "b18969c47003c095d5219e319e3a5ca4de13be2fd6e205ce2b38bde586b1a59b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
